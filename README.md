# Ryan Mulligan

**Staff AI Systems Architect** Â· Interpretability & Cognitive Architecture Research

---

Building **Lilly** â€” a self-steering cognitive AI running on Qwen3-8B with explicit internal state machinery:

- **8D Plutchik emotional field** â€” joy, trust, fear, surprise, sadness, disgust, anger, anticipation â€” with wave-packet interference for secondary emotion emergence
- **Evalatis** â€” evolutionary activation steering: vectors crystallize from activations, spawn children via crossover, compete and get pruned based on affinity Ã— staleness vs. surprise EMA
- **AffectiveResonator** â€” queries episodic memory for similar past situations, computes weighted valence, injects a blended steering vector at layers 14â€“18 during inference
- **163k-feature SAE transcoders** trained on Qwen3-8B MLP layers via TransformerLens
- **6-phase cognitive cycle** (Generate â†’ Curate â†’ Simulate â†’ Integrate â†’ Reflect â†’ Continue) with Active Inference / Free Energy Principle tracking prediction error across phases

The research question: can a model's behavior be continuously shaped by its own past emotional responses to similar situations? And if so, does it *know* this is happening?

This sits in direct conversation with mechanistic interpretability work on emergent introspection â€” building from the inside what that work probes from the outside.

---

**Stack:** Python Â· PyTorch Â· TransformerLens Â· SAELens Â· Letta Â· FalkorDB Â· Active Inference

**Open to roles in:** AI interpretability research Â· cognitive architectures Â· AI safety

---

ðŸ“¦ [`lilly-steering`](https://github.com/rmulligan/lilly-steering) â€” Core affect steering and evolutionary activation architecture
ðŸ“§ ryan@mulligan.dev
